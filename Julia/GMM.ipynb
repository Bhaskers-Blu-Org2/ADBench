{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:6af25a8e0491902990b91630d14dea1b2c097faedc1d6686718e885855a5fcc5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Base.Test\n",
      "using IPynbToJl\n",
      "ipynb_to_jl(\"GMM.ipynb\")\n",
      "ipynb_to_jl(\"logsumexp.ipynb\")\n",
      "ipynb_to_jl(\"LowerTriangular.ipynb\")\n",
      "\n",
      "@printf(\"julia version = %s, dir %s\\n\", Base.VERSION_STRING, pwd())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "include(\"MatVec.jl\")\n",
      "include(\"LowerTriangular.jl\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Conventional GMM.\n",
      "# This doesn't even use inverse covariance, because as soon as you\n",
      "# start down that route, you may as well go for lpGMM below.\n",
      "type GMM\n",
      "  n::Int           # number of Gaussians\n",
      "  d::Int           # dimension of Gaussian\n",
      "  alphas::Vec      # weights: n, require sum(alphas)==1\n",
      "  mus::Array{Vec}  # means: n, each dx1\n",
      "  sigmas::Array{SymMat}  # covariances: n, each dxd symmetric positive definite\n",
      "end\n",
      "\n",
      "function log_likelihood(g::GMM, x::Vec)\n",
      "  total = 0\n",
      "  for k=1:g.n\n",
      "    mean = g.mus[k]\n",
      "    weight =  g.alphas[k]\n",
      "    \u03a3 = g.sigmas[k]\n",
      "    mahalanobis = dot(mean - x, inv(\u03a3) * (mean - x))\n",
      "    total += weight / sqrt(det(2pi*full(\u03a3))) * exp(-0.5*mahalanobis)\n",
      "  end\n",
      "  log(total)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Test it a bit\n",
      "\n",
      "n=3\n",
      "d=2\n",
      "alphas=rand(n); alphas /= sum(alphas);\n",
      "mus=[randn(d) for k=1:n]\n",
      "sigmas=[AAt(randn(d,d)) for k=1:n]\n",
      "test_gmm = GMM(n,d,alphas,mus,sigmas)\n",
      "@printf(\"An example gmm = %s\\n\", test_gmm)\n",
      "\n",
      "x = randn(d) # Test point\n",
      "\n",
      "ll0 = log_likelihood(test_gmm, x)\n",
      "@printf(\"Tes log likelihood ll0=%f\\n\", ll0)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---------\n",
      "\n",
      "Log-parameterized GMM\n",
      "=====================\n",
      "\n",
      "This is a GMM parameterized by log-weights, and the Cholesky factor of the inverse covariance.  This means we can do unconstrained optimization, and in fact makes for a more efficient computation of the normalization constant, as well as more numerical stability all round.    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##########################################################################\n",
      "# Log-parametrized GMM.\n",
      "# Weights are strictly positive, covariances are parameterized by their inverse\n",
      "# square roots (lower triangular).\n",
      "type lpGMM\n",
      "  n::Int           # number of Gaussians\n",
      "  d::Int           # dimension of Gaussian\n",
      "  alphas::Vec      # log weights: n\n",
      "  mus::Array{Vec}  # means: n, each dx1\n",
      "  qs::Array{Vec}  # square-root-inverse-covariances, log(diagonal): n, each d x 1\n",
      "  Ls::Array{Vec}  # square-root-inverse-covariances, lower triangle: n, each d*(d-1)/2 x 1\n",
      "end\n",
      "\n",
      "# Convert simple GMM to lpGMM\n",
      "function lpGMM(g::GMM)\n",
      "  Ls = Array{Vec}(g.n)\n",
      "  qs = Array{Vec}(g.n)\n",
      "  for k=1:g.n\n",
      "    L = inv(chol(g.sigmas[k].data, Val{:L}))\n",
      "    q, L = ltri_pack(L)\n",
      "    qs[k], Ls[k] = vec(log(q)), vec(L)\n",
      "  end\n",
      "  lpGMM(g.n,g.d,log(g.alphas),g.mus,qs,Ls)\n",
      "end\n",
      "\n",
      "# Convert log-parameterized-GMM to simple GMM UnivariateGMM\n",
      "function GMM(l::lpGMM)\n",
      "  alphas::Vec = exp(l.alphas)/sum(exp(l.alphas))\n",
      "  mus::Array{Vec} = l.mus\n",
      "  Qs = [ltri_unpack(exp(l.qs[i]), l.Ls[i]) for i=1:l.n]\n",
      "  sigmas::Array{SymMat} = map(A->inv(Symmetric(A'*A)), Qs)\n",
      "  GMM(l.n,l.d,alphas,mus,sigmas)\n",
      "end\n",
      "\n",
      "g = lpGMM(test_gmm)\n",
      "@printf(\"testgmm=%s\\n**\\n\", test_gmm)\n",
      "@printf(\"gmm=%s\\n**\\n\", GMM(g))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "testgmm="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GMM(3,2,[0.16715818870968754,0.6724599360095661,0.1603818752807464],[[0.5242129123910795,-0.05805700620270187],[-1.2548187132585082,1.176074396609539],[1.076966044387117,-1.0954646269332933]],[\n",
        "[1.7773037758603347 1.0200467175983121\n",
        " 1.0200467175983121 1.3090802080404047],\n",
        "\n",
        "[0.46594100398235105 0.23840899714240615\n",
        " 0.23840899714240615 1.1469187368549634],\n",
        "\n",
        "[3.1920014691564744 1.1023985885948904\n",
        " 1.1023985885948904 0.42759159176563255]])\n",
        "**\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gmm=GMM(3,2,[0.16715818870968752,0.6724599360095661,0.1603818752807464],[[0.5242129123910795,-0.05805700620270187],[-1.2548187132585082,1.176074396609539],[1.076966044387117,-1.0954646269332933]],[\n",
        "[1.777303775860335 1.0200467175983123\n",
        " 1.0200467175983123 1.3090802080404047],\n",
        "\n",
        "[0.46594100398235094 0.23840899714240607\n",
        " 0.23840899714240607 1.1469187368549632],\n",
        "\n",
        "[3.1920014691564758 1.1023985885948908\n",
        " 1.1023985885948908 0.4275915917656327]])\n",
        "**\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "lpGMM log-likelihood\n",
      "--------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "const halflog2\u03c0 = log(2\u03c0)/2\n",
      "\n",
      "# Compute log-likelihood\n",
      "# This version is easy to read, but we can do better (see below)\n",
      "function log_likelihood_reference(g::lpGMM, x::Vec)\n",
      "  total = 0\n",
      "  weights = exp(g.alphas)\n",
      "  weights /= sum(weights) \n",
      "  for k=1:g.n\n",
      "    L_diagonal = exp(g.qs[k])\n",
      "    L_ltri_entries = g.Ls[k] \n",
      "    InvLowerTriangle = ltri_unpack(L_diagonal, L_ltri_entries)\n",
      "    mean = g.mus[k]\n",
      "    mahalanobis = sumsq(InvLowerTriangle * (mean - x))\n",
      "    total += weights[k] * det(InvLowerTriangle) * exp(-0.5*mahalanobis)\n",
      "  end\n",
      "  log(total) - halflog2\u03c0*g.d\n",
      "end\n",
      "\n",
      "ll1 = log_likelihood_reference(g, x)\n",
      "@printf(\"ll0=%f, ll1=%f, ratio to true=%f\\n\", ll0, ll1, ll0/ll1)\n",
      "@test_approx_eq_eps ll0 ll1 1e-12\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ll0="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-3.818412, ll1=-3.818412, ratio to true=1.000000\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "include(\"logsumexp.jl\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "logsumexp: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2.219867 = 2.219867\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "logsumexp_both (generic function with 1 method)"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Cleanest log_likelihood implementation\n",
      "function log_likelihood(g::lpGMM, x::Vec)\n",
      "    # function to combine log-diagonal an lower triangle\n",
      "    get_Q(L_log_diagonal, L_ltri_entries) = ltri_unpack(exp(L_log_diagonal), L_ltri_entries)\n",
      "\n",
      "    # mahalanobis distances squared\n",
      "    d_mahals = [0.5*sumsq(get_Q(g.qs[i], g.Ls[i])*(g.mus[i] - x)) for i in 1:g.n]\n",
      "    \n",
      "    # log determinants\n",
      "    log_determinants = [sum(g.qs[i]) for i in 1:g.n]\n",
      "    \n",
      "    logsumexp(g.alphas + log_determinants - d_mahals) - logsumexp(g.alphas) - halflog2\u03c0*g.d\n",
      "end\n",
      "\n",
      "ll2 = log_likelihood(g, x)\n",
      "@printf(\"ll0=%f, ll2=%f, ratio to true=%f\\n\", ll0, ll2, ll0/ll2)\n",
      "@test_approx_eq_eps ll0 ll2 1e-12\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ll0="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-3.818412, ll2=-3.818412, ratio to true=1.000000\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using ReverseDiffSource"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdiff(log_likelihood, (g,x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LoadError",
       "evalue": "LoadError: PCRE.exec error code -8\nwhile loading In[18], in expression starting on line 1",
       "output_type": "pyerr",
       "traceback": [
        "LoadError: PCRE.exec error code -8\nwhile loading In[18], in expression starting on line 1",
        "",
        " in error at error.jl:19"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:6b0c170f6e444c9946886a854a7e0e9488299bfca57bd3c60dc3fd2a8bc52777"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Base.Test\n",
      "\n",
      "@printf(\"julia version = %s, dir %s\\n\", Base.VERSION_STRING, pwd())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "julia version = "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.4.0-dev, dir C:\\projects\\autodiff\\julia\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "include(\"MatVec.jl\")\n",
      "include(\"LowerTriangular.jl\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LoadError",
       "evalue": "LoadError: could not open file C:\\projects\\autodiff\\julia\\ltri.jl\nwhile loading In[19], in expression starting on line 13",
       "output_type": "pyerr",
       "traceback": [
        "LoadError: could not open file C:\\projects\\autodiff\\julia\\ltri.jl\nwhile loading In[19], in expression starting on line 13",
        "",
        " in include at boot.jl:250",
        " in include_from_node1 at loading.jl:129"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Conventional GMM.\n",
      "# This doesn't even use inverse covariance, because as soon as you\n",
      "# start down that route, you may as well go for lpGMM below.\n",
      "type GMM\n",
      "  n::Int           # number of Gaussians\n",
      "  d::Int           # dimension of Gaussian\n",
      "  alphas::Vec      # weights: n, require sum(alphas)==1\n",
      "  mus::Array{Vec}  # means: n, each dx1\n",
      "  sigmas::Array{SymMat}  # covariances: n, each dxd symmetric positive definite\n",
      "end\n",
      "\n",
      "function log_likelihood(g::GMM, x::Vec)\n",
      "  total = 0\n",
      "  for k=1:g.n\n",
      "    mean = g.mus[k]\n",
      "    weight =  g.alphas[k]\n",
      "    \u03a3 = g.sigmas[k]\n",
      "    mahalanobis = dot(mean - x, inv(\u03a3) * (mean - x))\n",
      "    total += weight / sqrt(det(2pi*full(\u03a3))) * exp(-0.5*mahalanobis)\n",
      "  end\n",
      "  log(total)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "log_likelihood (generic function with 1 method)"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test it a bit\n",
      "\n",
      "n=3\n",
      "d=2\n",
      "alphas=rand(n); alphas /= sum(alphas);\n",
      "mus=[randn(d) for k=1:n]\n",
      "sigmas=[AAt(randn(d,d)) for k=1:n]\n",
      "test_gmm = GMM(n,d,alphas,mus,sigmas)\n",
      "@printf(\"An example gmm = %s\\n\", test_gmm)\n",
      "\n",
      "x = randn(d) # Test point\n",
      "\n",
      "ll0 = log_likelihood(test_gmm, x)\n",
      "@printf(\"Tes log likelihood ll0=%f\\n\", ll0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "An example gmm = "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GMM(3,2,[0.22936315626842912,0.031885680340370454,0.7387511633912004],[[-0.9346851324738641,-1.3175441142564603],[-0.1297659761438668,-0.7295282966735973],"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.6185466313837805,0.4800609156351103]],[\n",
        "["
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3.077023698294308 1.3080664694869901\n",
        " 1.3080664694869901 0.7729860958450397],\n",
        "\n",
        "[4.147399662574553 3.0992477478893026\n",
        " 3.0992477478893026 2.341450409182911],\n",
        "\n",
        "[0.0068392255013234446 -0.02117283545554069\n",
        " -0.02117283545554069 0.06616811559490114]])\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "m=1.4982201591629938, m=9.470231985490093, m=235310.599146671, w=0.002095\n",
        "ll0=-6.168300\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---------\n",
      "\n",
      "Log-parameterized GMM\n",
      "=====================\n",
      "\n",
      "This is a GMM parameterized by log-weights, and the Cholesky factor of the inverse covariance.  This means we can do unconstrained optimization, and in fact makes for a more efficient computation of the normalization constant, as well as more numerical stability all round.    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "include(\"LowerTriangular.jl\")  # get ltri_pack and ltri_unpack"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "An example lower triangle made from diag and LT=\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1.1 0.0 0.0 0.0\n",
        " 21.0 2.2 0.0 0.0\n",
        " 31.0 32.0 3.3 0.0\n",
        " 41.0 42.0 43.0 4.4]\n",
        "pak=([1.1,2.2,3.3,4.4],\n",
        "[21.0 31.0 32.0 41.0 42.0 43.0])\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##########################################################################\n",
      "# Log-parametrized GMM.\n",
      "# Weights are strictly positive, covariances are parameterized by their inverse\n",
      "# square roots (lower triangular).\n",
      "type lpGMM\n",
      "  n::Int           # number of Gaussians\n",
      "  d::Int           # dimension of Gaussian\n",
      "  alphas::Vec      # log weights: n\n",
      "  mus::Array{Vec}  # means: n, each dx1\n",
      "  LDs::Array{Vec}  # square-root-inverse-covariances, log(diagonal): n, each d x 1\n",
      "  LTs::Array{Vec}  # square-root-inverse-covariances, lower triangle: n, each d*(d-1)/2 x 1\n",
      "end\n",
      "\n",
      "# Convert simple GMM to lpGMM\n",
      "function lpGMM(g::GMM)\n",
      "  LTs = Array{Vec}(g.n)\n",
      "  LDs = Array{Vec}(g.n)\n",
      "  for k=1:g.n\n",
      "    L = inv(chol(g.sigmas[k].data, Val{:L}))\n",
      "    D, T = ltri_pack(L)\n",
      "    LDs[k], LTs[k] = vec(log(D)), vec(T)\n",
      "  end\n",
      "  lpGMM(g.n,g.d,log(g.alphas),g.mus,LDs,LTs)\n",
      "end\n",
      "\n",
      "# Convert log-parameterized-GMM to simple GMM UnivariateGMM\n",
      "function GMM(l::lpGMM)\n",
      "  alphas::Vec = exp(l.alphas)/sum(exp(l.alphas))\n",
      "  mus::Array{Vec} = l.mus\n",
      "  Ls = [ltri_unpack(exp(l.LDs[i]), l.LTs[i]) for i=1:l.n]\n",
      "  sigmas::Array{SymMat} = map(A->inv(Symmetric(A'*A)), Ls)\n",
      "  GMM(l.n,l.d,alphas,mus,sigmas)\n",
      "end\n",
      "\n",
      "g = lpGMM(test_gmm)\n",
      "\n",
      "#@printf(\"gmm=%s\\n**\\n\", GMM(g))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "lpGMM(3,2,[-1.4724486963034635,-3.4455982621249777,-0.30279413545264555],[[-0.9346851324738641,-1.3175441142564603],[-0.1297659761438668,-0.7295282966735973],[0.6185466313837805,0.4800609156351103]],[[-0.5619813990316764,0.7641203205465874],[-0.7112407752616458,1.8353174771525387],[2.492540392279717,3.691790417194345]],[[-0.9127507761336187],[-4.683264205924887],[124.1927676579722]])"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "lpGMM log-likelihood\n",
      "--------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "const halflog2\u03c0 = log(2\u03c0)/2\n",
      "\n",
      "# Compute log-likelihood\n",
      "# This version is easy to read, but we can do better (see below)\n",
      "function log_likelihood_reference(g::lpGMM, x::Vec)\n",
      "  total = 0\n",
      "  weights = exp(g.alphas)\n",
      "  weights /= sum(weights) \n",
      "  for k=1:g.n\n",
      "    L_diagonal = exp(g.LDs[k])\n",
      "    L_ltri_entries = g.LTs[k] \n",
      "    InvLowerTriangle = ltri_unpack(L_diagonal, L_ltri_entries)\n",
      "    mean = g.mus[k]\n",
      "    mahalanobis = sumsq(InvLowerTriangle * (mean - x))\n",
      "    total += weights[k] * det(InvLowerTriangle) * exp(-0.5*mahalanobis)\n",
      "  end\n",
      "  log(total) - halflog2\u03c0*g.d\n",
      "end\n",
      "\n",
      "ll1 = log_likelihood_reference(g, x)\n",
      "@printf(\"ll0=%f, ll1=%f, ratio to true=%f\\n\", ll0, ll1, ll0/ll1)\n",
      "@test_approx_eq_eps ll0 ll1 1e-12\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ll0="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-6.168300, ll1=-6.168300, ratio to true=1.000000\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using IPynbToJl\n",
      "ipynb_to_jl(\"logsumexp.ipynb\")\n",
      "include(\"logsumexp.jl\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "logsumexp: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2.308313 = 2.308313\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "logsumexp_both (generic function with 1 method)"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute norm(Lx)^2\n",
      "normAx(L::Mat, v::Vec) = sumsq(L*v)\n",
      "\n",
      "# Cleanest log_likelihood implementation\n",
      "function log_likelihood(g::lpGMM, x::Vec)\n",
      "    get_L(L_log_diagonal, L_ltri_entries) = ltri_unpack(exp(L_log_diagonal), L_ltri_entries)\n",
      "\n",
      "    d_mahal = [0.5*normAx(get_L(g.LDs[i], g.LTs[i]), g.mus[i] - x) for i in 1:g.n]\n",
      "    \n",
      "    determinants = [sum(g.LDs[i]) for i in 1:g.n]\n",
      "    \n",
      "    logsumexp(g.alphas + determinants - d_mahal) - logsumexp(g.alphas) - halflog2\u03c0*g.d\n",
      "end\n",
      "\n",
      "ll2 = log_likelihood(g, x)\n",
      "@printf(\"ll0=%f, ll2=%f, rat=%f\\n\", ll0, ll2, ll0/ll2)\n",
      "@test_approx_eq_eps ll0 ll2 1e-12\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ll0="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-6.168300, ll2=-6.168300, rat=1.000000\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
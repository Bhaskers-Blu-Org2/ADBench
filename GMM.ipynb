{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:bb65edc1e7a78ec6ce553bb3001d132b414d43a106da3b8c7e0458461550cd9b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Base.Test\n",
      "using IPynbToJl\n",
      "ipynb_to_jl(\"logsumexp.ipynb\")\n",
      "ipynb_to_jl(\"LowerTriangular.ipynb\")\n",
      "\n",
      "@printf(\"julia version = %s, dir %s\\n\", Base.VERSION_STRING, pwd())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote logsumexp.jl\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wrote LowerTriangular.jl\n",
        "julia version = 0.4.0-dev, dir C:\\dev\\GitHub\\autodiff\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "include(\"MatVec.jl\")\n",
      "include(\"LowerTriangular.jl\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "An example lower triangle made from diag and LT=\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "["
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.1 0.0 0.0 0.0\n",
        " 21.0 2.2 0.0 0.0\n",
        " 31.0 32.0 3.3 0.0\n",
        " 41.0 42.0 43.0 4.4]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "packed=([1.1,2.2,3.3,4.4],\n",
        "[21.0 31.0 32.0 41.0 42.0 43.0])\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Conventional GMM.\n",
      "# This doesn't even use inverse covariance, because as soon as you\n",
      "# start down that route, you may as well go for lpGMM below.\n",
      "type GMM\n",
      "  n::Int           # number of Gaussians\n",
      "  d::Int           # dimension of Gaussian\n",
      "  alphas::Vec      # weights: n, require sum(alphas)==1\n",
      "  mus::Array{Vec}  # means: n, each dx1\n",
      "  sigmas::Array{SymMat}  # covariances: n, each dxd symmetric positive definite\n",
      "end\n",
      "\n",
      "function log_likelihood(g::GMM, x::Vec)\n",
      "  total = 0\n",
      "  for k=1:g.n\n",
      "    mean = g.mus[k]\n",
      "    weight =  g.alphas[k]\n",
      "    \u03a3 = g.sigmas[k]\n",
      "    mahalanobis = dot(mean - x, inv(\u03a3) * (mean - x))\n",
      "    total += weight / sqrt(det(2pi*full(\u03a3))) * exp(-0.5*mahalanobis)\n",
      "  end\n",
      "  log(total)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "log_likelihood (generic function with 1 method)"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test it a bit\n",
      "\n",
      "n=3\n",
      "d=2\n",
      "alphas=rand(n); alphas /= sum(alphas);\n",
      "mus=[randn(d) for k=1:n]\n",
      "sigmas=[AAt(randn(d,d)) for k=1:n]\n",
      "test_gmm = GMM(n,d,alphas,mus,sigmas)\n",
      "@printf(\"An example gmm = %s\\n\", test_gmm)\n",
      "\n",
      "x = randn(d) # Test point\n",
      "\n",
      "ll0 = log_likelihood(test_gmm, x)\n",
      "@printf(\"Tes log likelihood ll0=%f\\n\", ll0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "An example gmm = "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GMM(3,2,[0.9233153942661779,0.012768940463009434,0.06391566527081266],[[-0.14154285609361175,-1.6576824967350574],[-0.1009292868051995,1.6053410578475171],[-1.2234101302360822,-1.0847615476982166]],"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[\n",
        "["
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3.762023441989988 -0.5784564229025504\n",
        " -0.5784564229025504 0.0890041450191498],\n",
        "\n",
        "[1.2365894088055693 0.35400981899391826\n",
        " 0.35400981899391826 0.6996368496253054],\n",
        "\n",
        "[2.040230800922826 1.7009901800956477\n",
        " 1.7009901800956477 5.412087828740088]])\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tes log likelihood ll0=-5.752023\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---------\n",
      "\n",
      "Log-parameterized GMM\n",
      "=====================\n",
      "\n",
      "This is a GMM parameterized by log-weights, and the Cholesky factor of the inverse covariance.  This means we can do unconstrained optimization, and in fact makes for a more efficient computation of the normalization constant, as well as more numerical stability all round.    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "include(\"LowerTriangular.jl\")  # get ltri_pack and ltri_unpack"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "An example lower triangle made from diag and LT=\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1.1 0.0 0.0 0.0\n",
        " 21.0 2.2 0.0 0.0\n",
        " 31.0 32.0 3.3 0.0\n",
        " 41.0 42.0 43.0 4.4]\n",
        "packed=([1.1,2.2,3.3,4.4],\n",
        "[21.0 31.0 32.0 41.0 42.0 43.0])\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##########################################################################\n",
      "# Log-parametrized GMM.\n",
      "# Weights are strictly positive, covariances are parameterized by their inverse\n",
      "# square roots (lower triangular).\n",
      "type lpGMM\n",
      "  n::Int           # number of Gaussians\n",
      "  d::Int           # dimension of Gaussian\n",
      "  alphas::Vec      # log weights: n\n",
      "  mus::Array{Vec}  # means: n, each dx1\n",
      "  LDs::Array{Vec}  # square-root-inverse-covariances, log(diagonal): n, each d x 1\n",
      "  LTs::Array{Vec}  # square-root-inverse-covariances, lower triangle: n, each d*(d-1)/2 x 1\n",
      "end\n",
      "\n",
      "# Convert simple GMM to lpGMM\n",
      "function lpGMM(g::GMM)\n",
      "  LTs = Array{Vec}(g.n)\n",
      "  LDs = Array{Vec}(g.n)\n",
      "  for k=1:g.n\n",
      "    L = inv(chol(g.sigmas[k].data, Val{:L}))\n",
      "    D, T = ltri_pack(L)\n",
      "    LDs[k], LTs[k] = vec(log(D)), vec(T)\n",
      "  end\n",
      "  lpGMM(g.n,g.d,log(g.alphas),g.mus,LDs,LTs)\n",
      "end\n",
      "\n",
      "# Convert log-parameterized-GMM to simple GMM UnivariateGMM\n",
      "function GMM(l::lpGMM)\n",
      "  alphas::Vec = exp(l.alphas)/sum(exp(l.alphas))\n",
      "  mus::Array{Vec} = l.mus\n",
      "  Ls = [ltri_unpack(exp(l.LDs[i]), l.LTs[i]) for i=1:l.n]\n",
      "  sigmas::Array{SymMat} = map(A->inv(Symmetric(A'*A)), Ls)\n",
      "  GMM(l.n,l.d,alphas,mus,sigmas)\n",
      "end\n",
      "\n",
      "g = lpGMM(test_gmm)\n",
      "\n",
      "#@printf(\"gmm=%s\\n**\\n\", GMM(g))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "lpGMM(3,2,[-0.0797843972501872,-4.360739583170133,-2.7501907947358455],[[-0.14154285609361175,-1.6576824967350574],[-0.1009292868051995,1.6053410578475171],[-1.2234101302360822,-1.0847615476982166]],[[-0.6624784810583655,4.86475281739375],[-0.10617855666788792,0.25683884013072344],[-0.3565314695819025,-0.6923879579738514]],[[19.933541041006684],[-0.3701122643634175],[-0.41717880640138805]])"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "lpGMM log-likelihood\n",
      "--------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "const halflog2\u03c0 = log(2\u03c0)/2\n",
      "\n",
      "# Compute log-likelihood\n",
      "# This version is easy to read, but we can do better (see below)\n",
      "function log_likelihood_reference(g::lpGMM, x::Vec)\n",
      "  total = 0\n",
      "  weights = exp(g.alphas)\n",
      "  weights /= sum(weights) \n",
      "  for k=1:g.n\n",
      "    L_diagonal = exp(g.LDs[k])\n",
      "    L_ltri_entries = g.LTs[k] \n",
      "    InvLowerTriangle = ltri_unpack(L_diagonal, L_ltri_entries)\n",
      "    mean = g.mus[k]\n",
      "    mahalanobis = sumsq(InvLowerTriangle * (mean - x))\n",
      "    total += weights[k] * det(InvLowerTriangle) * exp(-0.5*mahalanobis)\n",
      "  end\n",
      "  log(total) - halflog2\u03c0*g.d\n",
      "end\n",
      "\n",
      "ll1 = log_likelihood_reference(g, x)\n",
      "@printf(\"ll0=%f, ll1=%f, ratio to true=%f\\n\", ll0, ll1, ll0/ll1)\n",
      "@test_approx_eq_eps ll0 ll1 1e-12\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ll0="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-5.752023, ll1=-5.752023, ratio to true=1.000000\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "include(\"logsumexp.jl\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "logsumexp: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.946713 = 1.946713\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "logsumexp_both (generic function with 1 method)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Cleanest log_likelihood implementation\n",
      "function log_likelihood(g::lpGMM, x::Vec)\n",
      "    # function to combine log-diagonal an lower triangle\n",
      "    get_L(L_log_diagonal, L_ltri_entries) = ltri_unpack(exp(L_log_diagonal), L_ltri_entries)\n",
      "\n",
      "    # mahalanobis distances squared\n",
      "    d_mahals = [0.5*sumsq(get_L(g.LDs[i], g.LTs[i])*(g.mus[i] - x)) for i in 1:g.n]\n",
      "    \n",
      "    # log determinants\n",
      "    log_determinants = [sum(g.LDs[i]) for i in 1:g.n]\n",
      "    \n",
      "    logsumexp(g.alphas + log_determinants - d_mahals) - logsumexp(g.alphas) - halflog2\u03c0*g.d\n",
      "end\n",
      "\n",
      "ll2 = log_likelihood(g, x)\n",
      "@printf(\"ll0=%f, ll2=%f, ratio to true=%f\\n\", ll0, ll2, ll0/ll2)\n",
      "@test_approx_eq_eps ll0 ll2 1e-12\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ll0="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-5.752023, ll2=-5.752023, rat=1.000000\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}